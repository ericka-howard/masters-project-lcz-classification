---
title: "07_iterating-over-tuning-parameters"
author: "Ericka B. Smith"
date: "2/14/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(randomForest)
library(tidyverse)
library(here)
library(raster)
library(rgdal)
library(purrr)
library(mapproj)
library(forcats)
source(here("R", "get_oa_metrics.R"))
source(here("R", "get_f1_score.R"))
source(here("R", "predict_lcz.R"))
```

## Load Data

```{r}
train <- readRDS(here("results", "train.rds"))
test <- readRDS(here("results", "test.rds"))
```

# Start with default mtry=6

## Create & Evaluate Model

```{r}
rf <- randomForest(lcz ~ ., data=train, ntree=50, keep.inbag = T)
```
## Calculate OOB Error rate

```{r}
# print(rf)
# # gets list of OOB prediction for each row (each pixel observation)
# ind_tree_preds <- predict(rf, predict.all=TRUE)
# # checks when OOB prediction is the same as true LCZ value
# correct <- ind_tree_preds == train$lcz
# # calculate OOB error rate
# (length(correct)-sum(correct))/length(correct)
```
OA is the same as 1-OOB Error Rate.

Now with different parameters.

```{r}
try_parameter <- function(dat, ...){
  rf <- randomForest(lcz ~ .,
                     data=dat,
                     ...,
                     keep.inbag = T)
  predict(rf, predict.all=T)
  oob_df <- tibble(lcz = dat$lcz, 
                       lcz_predicted = predict(rf, predict.all=T)) %>%
    drop_na()
  oas <- get_oa_metrics(oob_df)
  f1 <- get_f1_score(oob_df, true=lcz, predicted = lcz_predicted)
  return(list(oas, f1))
}
```

# almost all the way there but need to get it to save the name in some way so that I can keep track of which parameter goes with which accuracy vals

```{r}
metrics <- try_parameter(train, mtry=5)

bind_rows(current_oa_dat, "name" = metrics[[1]])
bind_cols(current_f1_dat, metrics[[2]])
```



