---
title: "07_iterating-over-tuning-parameters"
author: "Ericka B. Smith"
date: "2/14/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(randomForest)
library(tidyverse)
library(here)
library(raster)
library(rgdal)
library(purrr)
library(mapproj)
source(here("R", "get_oa_metrics.R"))
source(here("R", "get_f1_score.R"))
source(here("R", "predict_lcz.R"))
```

## Load Data

```{r}
train <- readRDS(here("results", "train.rds"))
test <- readRDS(here("results", "test.rds"))
```

# Start with default mtry=6

## Create & Evaluate Model

```{r}
rf <- randomForest(lcz ~ ., data=train, ntree=5, keep.inbag = T, keep.forest=T)
print(rf)
```


This gets a list of how each tree predicts each pixel. the newdata=train thing doesn't entirely make sense to me but it looks like it's just how it has to be. [1](https://stats.stackexchange.com/questions/253351/predict-function-tuning-for-random-forest)

```{r}
inbag <- rf$inbag
ind_tree_preds <- as.data.frame(predict(rf, newdata=train, predict.all=TRUE)$individual)
```



```{r}
bag <- inbag[,2]
index_vals <- as.numeric(names(bag[which(bag==0)]))
lcz_predicted <- ind_tree_preds[index_vals, 2]
oob_prediction <- cbind(train[index_vals,], lcz_predicted)
oas <- get_oa_metrics(oob_prediction)
```




original
```{r}
getting_accuracy_metrics_oob <- function(x){
  bag <- inbag[,x]
  index_vals <- as.numeric(names(bag[which(bag==0)]))
  lcz_predicted <- ind_tree_preds[index_vals, x]
  oob_prediction <- cbind(train[index_vals,], lcz_predicted)
  oas <- get_oa_metrics(oob_prediction)
  return(oas)
  }
```


```{r}
metrics_galore <- map_dfr(1:5, ~getting_accuracy_metrics_oob(.x))
```

look here for oob error info [here](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)

