---
title: "Walk through of Chapter 5: Classification Resource"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Following along with [Chapter 5: Classification](http://ceholden.github.io/open-geo-tutorial/R/chapter_5_classification.html) from [Open Source Geoprocessing Tutorial by Chris Holden](https://github.com/ceholden/open-geo-tutorial)

``` {r}
library(randomForest)
library(raster)
library(rgdal)
if (file.exists('training_data.shp') == F) {
    download.file(url = 'https://raw.githubusercontent.com/ceholden/open-geo-tutorial/master/example/training_data.zip', 
                  destfile = 'training_data.zip', method = 'curl')
    unzip('training_data.zip')
}
training <- readOGR('training_data.shp', layer='training_data')
if (file.exists('LE70220491999322EDC01_stack.gtif') == F) {
    download.file(url='https://raw.githubusercontent.com/ceholden/open-geo-tutorial/master/example/LE70220491999322EDC01_stack.gtif',
                  destfile='LE70220491999322EDC01_stack.gtif', method='curl')
}
le7 <- brick('LE70220491999322EDC01_stack.gtif')
```

* Had to use comment by mysteRious on [stack overflow](https://stackoverflow.com/questions/35666638/cant-access-user-library-in-r-non-zero-exit-status-warning) to get `rgdal` to install. 
* `brick()` ["Create a RasterBrick Object: A RasterBrick is a multi-layer raster object. They are typically created from a multi-band file; but they can also exist entirely in memory. "](https://www.rdocumentation.org/packages/raster/versions/1.7-6/topics/brick)


Extract the reflectance data covering the training data labels.


``` {r}
roi_data <- extract(le7, training, df=TRUE)
```

* `extract()` ["Extract values from a Raster* object at the locations of spatial vector data. There are methods for points, lines, and polygons (classes from `sp` or `sf`), for a matrix or data.frame of points. You can also use cell numbers and Extent (rectangle) objects to extract values"](https://www.rdocumentation.org/packages/raster/versions/3.4-5/topics/extract)


Mask out clouds or cloud shadows: 

``` {r}
roi_data[which(roi_data$Band.8 > 1)] <- NA
```

Also need to attach the labels to this `DataFrame`:

``` {r}
roi_data$lc <- as.factor(training$id[roi_data$ID])
roi_data$desc <- as.factor(training$class[roi_data$ID])
```

### Train RandomForest

Let's use the extracted training data to train the RandomForest algorithm:

``` {r}
# Set seed value for RNG for reproducibility
set.seed(1234567890)
colnames(roi_data)
# Shorten column names
colnames(roi_data) <- c('ID', 'b1', 'b2', 'b3', 'b4', 'b5', 'b7', 'b6', 'fmask', 'lc', 'desc')
rf <- randomForest(lc ~ b1 + b2 + b3 + b4 + b5 + b7 + b6, data=roi_data, importance=TRUE)
print(rf)
```

One of the useful metrics provided by RandomForest is the `importance` metric. This metric can be computed in two ways -- the mean decrease in accuracy as each variable is removed or the [Gini impurity](http://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity) metric. Either way it gives an idea of what features (Landsat bands) were important in assigning the classification labels.

```{r}
importance(rf)
```

See `?importance` for more information:

> Here are the definitions of the variable importance measures. The first measure is computed from permuting OOB data: For each tree, the prediction error on the out-of-bag portion of the data is recorded (error rate for classification, MSE for regression). Then the same is done after permuting each predictor variable. The difference between the two are then averaged over all trees, and normalized by the standard deviation of the differences. If the standard deviation of the differences is equal to 0 for a variable, the division is not done (but the average is almost always equal to 0 in that case).
> The second measure is the total decrease in node impurities from splitting on the variable, averaged over all trees. For classification, the node impurity is measured by the Gini index. For regression, it is measured by residual sum of squares.

``` {r}
varImpPlot(rf)
```

Sticking with similarities for right now, we see that the thermal band (band 6) is not very useful for our classification. This should agree with our expectations because the brightness temperature does not vary greatly across the land covers we selected, although it can prove useful when comparing irregated agriculture against natural grasslands or urban heat island impacted development against more suburban development.

I am more inclined to believe the importance as measured by the mean decrease in the Gini impurity metric because it ranks the two short-wave infrared (SWIR) bands more highly than the near-infrared (NIR) or the visible. Because it is impacted by leaf moisture, the SWIR bands greatly help the distinction between forests and grasslands. One thing to note is that because many of the bands are intrinsically or physically correlated with one another, our dataset is highly collinear. This multicollinearity means that a small subset of the bands provide truely unique information and our importance metric will be unstable depending on the random chance of one band being favored over another highly correlated band.

### Classify

We can proceed to classify the entire raster now that our algorithm is trained. To make sure our classifier knows what raster bands to choose from, we'll begin by renaming the layers within our Landsat 7 image.

``` {r}
le7_class <- le7
names(le7_class) <- c('b1', 'b2', 'b3', 'b4', 'b5', 'b7', 'b6', 'fmask')
# Predict!
le7_pred <- predict(le7_class, model=rf, na.rm=T)
```

### Map

Let's make a map!

``` {r}
# Create color map
colors <- c(rgb(0, 150, 0, maxColorValue=255),  # Forest
            rgb(0, 0, 255, maxColorValue=255),  # Water
            rgb(0, 255, 0, maxColorValue=255),  # Herbaceous
            rgb(160, 82, 45, maxColorValue=255),  # Barren
            rgb(255, 0, 0, maxColorValue=255))  # Urban
plotRGB(le7, r=5, g=4, b=3, stretch="lin")
plot(le7_pred, col=colors)
```

