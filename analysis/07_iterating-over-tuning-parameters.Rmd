---
title: "07_iterating-over-tuning-parameters"
author: "Ericka B. Smith"
date: "2/14/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(randomForest)
library(tidyverse)
library(here)
library(raster)
library(rgdal)
library(purrr)
library(mapproj)
library(forcats)
source(here("R", "get_oa_metrics.R"))
source(here("R", "get_f1_score.R"))
source(here("R", "predict_lcz.R"))
```

## Load Data

```{r}
train <- readRDS(here("results", "train.rds"))
test <- readRDS(here("results", "test.rds"))
```

# Start with default mtry=6

## Create & Evaluate Model

```{r}
rf <- randomForest(lcz ~ ., data=train, ntree=50, keep.inbag = T)
```
## Calculate OOB Error rate

```{r}
# print(rf)
# # gets list of OOB prediction for each row (each pixel observation)
# ind_tree_preds <- predict(rf, predict.all=TRUE)
# # checks when OOB prediction is the same as true LCZ value
# correct <- ind_tree_preds == train$lcz
# # calculate OOB error rate
# (length(correct)-sum(correct))/length(correct)
```
OA is the same as 1-OOB Error Rate.

Now with different parameters.

```{r}
try_parameter <- function(dat, response, ...)

rf <- randomForest(lcz ~ ., 
                    data=dat, 
                    ...,
                    keep.inbag = T)
predict(rf, predict.all=T)
oob_df <- data.frame(lcz = dat$lcz , 
                     lcz_predicted = predict(rf, predict.all=T))

oas <- get_oa_metrics(oob_df)
f1 <- get_f1_score(oob_df, true=lcz, predicted = lcz_predicted)
metrics <- list(param_val, oas, f1)
return(metrics)
```



```{r}
#metrics_galore <- map_dfr(1:5, ~getting_accuracy_metrics_oob(.x))
```



