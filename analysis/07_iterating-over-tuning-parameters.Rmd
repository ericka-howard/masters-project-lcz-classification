---
title: "07_iterating-over-tuning-parameters"
author: "Ericka B. Smith"
date: "2/14/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(randomForest)
library(tidyverse)
library(here)
library(raster)
library(rgdal)
library(purrr)
library(mapproj)
library(forcats)
source(here("R", "get_oa_metrics.R"))
source(here("R", "get_f1_score.R"))
source(here("R", "predict_lcz.R"))
```

## Load Data

```{r}
train <- readRDS(here("results", "train.rds"))
test <- readRDS(here("results", "test.rds"))
```

# Start with default mtry=6

## Create & Evaluate Model

```{r}
ntrees <- 500
rf <- randomForest(lcz ~ ., data=train, ntree=ntrees, keep.inbag = T, keep.forest=T)
print(rf)
```
## Calculate OOB Error rate

```{r}
# inbag <- rf$inbag
# gets list of OOB prediction for each row (each pixel observation)
ind_tree_preds <- predict(rf, predict.all=TRUE)
# checks when OOB prediction is the same as true LCZ value
correct <- ind_tree_preds == train$lcz
# calculate OOB error rate
(length(correct)-sum(correct))/length(correct)
```

## Apply to OA

Looking at it, OA is the same as 1-OOB Error Rate. Let's try.

```{r}
oob_df <- data.frame(lcz = train$lcz, 
                        lcz_predicted = ind_tree_preds)
oas <- get_oa_metrics(oob_df)
1-oas$oa
```

Yep it's the same.

Do I have any reason to expect that oa_nat or oa_urb are not only going to be especially different, but also that varying parameters in `randomForest()` is going to interact differently with either?

Probably not. Maybe for F1 score though..

Format of these iterations should be similar either way.


(Saving for later)

```{r}
#metrics_galore <- map_dfr(1:5, ~getting_accuracy_metrics_oob(.x))
```



